{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "406e6475",
   "metadata": {},
   "source": [
    "# 1.What is the purpose of text preprocessing in NLP, and why is it essential before analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5ab40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Noise Reduction:\n",
    "\n",
    "Raw text data often contains noise, such as special characters, punctuation, and irrelevant symbols.\n",
    "Preprocessing helps remove these elements, reducing interference in subsequent analyses.\n",
    "\n",
    "2.Normalization:\n",
    "\n",
    "Text data may have variations in case, spelling, or representation of words. \n",
    "Normalization ensures consistency by converting text to a standard format (e.g., converting all text to lowercase)\n",
    "and addressing issues like stemming and lemmatization.\n",
    "\n",
    "3.Tokenization:\n",
    "\n",
    "Tokenization involves breaking down text into smaller units, such as words or phrases (tokens).\n",
    "This step is crucial for many NLP tasks, as it provides the basic units for analysis.\n",
    "\n",
    "4.Stopword Removal:\n",
    "\n",
    "Stopwords are common words (e.g., \"the,\" \"and,\" \"is\") that often don't carry much meaning and can be removed \n",
    "to focus on more meaningful content. Removing stopwords reduces the dimensionality of the data and can improve \n",
    "the efficiency of analysis.\n",
    "\n",
    "5.Removing HTML Tags and Special Characters:\n",
    "\n",
    "In web-based applications, text data may contain HTML tags or special characters.\n",
    "Removing these elements is essential for extracting the actual content of the text.\n",
    "\n",
    "6.Handling Contractions and Abbreviations:\n",
    "\n",
    "Preprocessing helps address contractions (e.g., \"can't\" to \"cannot\") and abbreviations,\n",
    "ensuring uniformity in the representation of words.\n",
    "\n",
    "7.Handling Missing Data:\n",
    "\n",
    "Text data may have missing values or incomplete sentences. \n",
    "Text preprocessing can involve handling missing data to ensure the quality and completeness of the dataset.\n",
    "\n",
    "8.Vectorization:\n",
    "\n",
    "Many NLP algorithms and models require numerical input. Text preprocessing involves converting \n",
    "text into numerical representations, such as word embeddings or bag-of-words vectors.\n",
    "\n",
    "9.Feature Engineering:\n",
    "\n",
    "Additional features, such as sentiment scores, can be derived from text during preprocessing, \n",
    "providing valuable information for analysis.\n",
    "\n",
    "10.Improved Model Performance:\n",
    "\n",
    "Preprocessing contributes to the overall performance of NLP models. A well-preprocessed\n",
    "dataset ensures that the models can focus on extracting meaningful patterns from the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d710cef3",
   "metadata": {},
   "source": [
    "# 2.Describe tokenization in NLP and explain its significance in text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2b607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenization is the process of breaking down a text into smaller units, such as words, phrases, or sentences. \n",
    "These smaller units are called tokens. Tokenization is a fundamental step in natural language processing (NLP) \n",
    "and plays a crucial role in various NLP tasks. The significance of tokenization in text processing lies in its \n",
    "ability to convert unstructured text into a structured format, making it easier to analyze and extract meaningful information.\n",
    "\n",
    "Significance of Tokenization in Text Processing:\n",
    "\n",
    "1.Text Analysis:\n",
    "\n",
    "Tokenization provides the basic units (tokens) for further analysis.\n",
    "It allows you to examine the frequency of words, identify patterns, and gain insights into the structure of the text.\n",
    "\n",
    "2.Feature Extraction:\n",
    "\n",
    "In machine learning, tokenization is a crucial step in feature extraction. \n",
    "It converts text into a format that can be used as input for machine learning models, such as bag-of-words or word embeddings.\n",
    "\n",
    "3.Text Classification:\n",
    "\n",
    "Tokenization is essential for tasks like text classification, where the presence or absence of specific words (tokens) \n",
    "contributes to the classification of the text into predefined categories.\n",
    "\n",
    "4.Search Engines:\n",
    "\n",
    "In search engines, tokenization is used to index and retrieve documents efficiently. \n",
    "Each token becomes a key term that facilitates searching and ranking.\n",
    "\n",
    "5.Named Entity Recognition (NER):\n",
    "\n",
    "NER tasks involve identifying and classifying entities (e.g., names, locations) in text.\n",
    "Tokenization helps in breaking down the text into units for accurate entity recognition.\n",
    "\n",
    "6.Text Summarization:\n",
    "\n",
    "Tokenization aids in identifying key phrases and sentences, making it easier to generate concise summaries of longer texts.\n",
    "\n",
    "7.Language Modeling:\n",
    "\n",
    "Tokenization is a crucial step in building language models, where sequences of tokens are used to predict the next\n",
    "word in a sentence.\n",
    "\n",
    "8.Information Retrieval:\n",
    "\n",
    "Tokenization facilitates the retrieval of relevant information by breaking down text into units that can be \n",
    "matched with user queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "513fdd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\TmC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokens: ['Tokenization', 'is', 'a', 'key', 'step', 'in', 'NLP', '.', 'It', 'breaks', 'down', 'text', 'into', 'smaller', 'units', 'like', 'words', 'or', 'sentences', '.']\n",
      "Sentence tokens: ['Tokenization is a key step in NLP.', 'It breaks down text into smaller units like words or sentences.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the Punkt tokenizer model for English\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Now you can proceed with your tokenization code\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"Tokenization is a key step in NLP. It breaks down text into smaller units like words or sentences.\"\n",
    "\n",
    "# Tokenize into words\n",
    "tokens_words = word_tokenize(text)\n",
    "print(\"Word tokens:\", tokens_words)\n",
    "\n",
    "# Tokenize into sentences\n",
    "tokens_sentences = sent_tokenize(text)\n",
    "print(\"Sentence tokens:\", tokens_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542a723c",
   "metadata": {},
   "source": [
    "# 3.What are the differences between stemming and lemmatization in NLP? When would you choose one over the other?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f84dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stemming and lemmatization are both techniques used in natural language processing (NLP) to reduce \n",
    "words to their base or root form. However, they operate in slightly different ways, and the choice between \n",
    "them depends on the specific requirements of the task.\n",
    "\n",
    "Stemming:\n",
    "\n",
    "    Stemming involves removing prefixes or suffixes from words to obtain their root form. \n",
    "    The resulting stems may not be actual words, but they represent the core meaning of the word.\n",
    "\n",
    "Lemmatization:    \n",
    "    \n",
    "    Lemmatization, on the other hand, involves reducing words to their base or dictionary form (lemma). \n",
    "    The lemmatized form is a valid word, making it more interpretable than stemming.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a477384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed words: ['stem', 'is', 'a', 'techniqu', 'use', 'in', 'natur', 'languag', 'process', '.', 'it', 'simplifi', 'word', 'to', 'their', 'root', 'form', '.']\n"
     ]
    }
   ],
   "source": [
    "#Stemming:\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"Stemming is a technique used in natural language processing. It simplifies words to their root form.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Create a PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# Apply stemming to each token\n",
    "stemmed_words = [porter_stemmer.stem(word) for word in tokens]\n",
    "\n",
    "print(\"Stemmed words:\", stemmed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f77d8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\TmC\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized words: ['Lemmatization', 'is', 'a', 'technique', 'used', 'in', 'natural', 'language', 'processing', '.', 'It', 'reduces', 'word', 'to', 'their', 'base', 'or', 'dictionary', 'form', '.']\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization:  \n",
    "import nltk\n",
    "\n",
    "# Download the WordNet data\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"Lemmatization is a technique used in natural language processing. It reduces words to their base or dictionary form.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Create a WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply lemmatization to each token\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "print(\"Lemmatized words:\", lemmatized_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe6bf48",
   "metadata": {},
   "source": [
    "# 4.Explain the concept of stop words and their role in text preprocessing. How do they impact NLP tasks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d42748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stop words are common words that are often filtered out during text preprocessing in \n",
    "natural language processing (NLP) because they are considered to be of little value in terms \n",
    "of information content. These words are very common across all languages and don't carry \n",
    "significant meaning by themselves. Examples of stop words include \"the,\" \"and,\" \"is,\" \"in,\" etc.\n",
    "\n",
    "The role of stop words in text preprocessing includes the following aspects:\n",
    "\n",
    "1.Noise Reduction:\n",
    "\n",
    "Stop words are frequently occurring words that don't contribute much to the meaning of a document. \n",
    "Removing them helps reduce noise in the text data, making it easier to focus on the more meaningful words.\n",
    "\n",
    "2.Dimensionality Reduction:\n",
    "\n",
    "Removing stop words reduces the number of unique words in a document, which helps in reducing the \n",
    "dimensionality of the data. This can be beneficial for computational efficiency and resource usage.\n",
    "\n",
    "3.Focus on Content Words:\n",
    "\n",
    "By eliminating common stop words, the remaining words in the text are often more content-rich and \n",
    "contribute more meaning to the document. This is particularly useful in tasks like information retrieval and text analysis.\n",
    "\n",
    "4.Improved Performance in Certain NLP Tasks:\n",
    "\n",
    "In some NLP tasks, such as sentiment analysis or document classification, removing stop words can lead to \n",
    "improved model performance. Stop words may not carry sentiment or topic-specific information, so excluding \n",
    "them can enhance the model's ability to capture relevant patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84ee88df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TmC\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens: ['Stop', 'words', 'are', 'common', 'words', 'that', 'are', 'often', 'filtered', 'out', 'during', 'text', 'preprocessing', 'in', 'natural', 'language', 'processing', '.']\n",
      "Filtered tokens without stop words: ['Stop', 'words', 'common', 'words', 'often', 'filtered', 'text', 'preprocessing', 'natural', 'language', 'processing', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK stop words data\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample text\n",
    "text = \"Stop words are common words that are often filtered out during text preprocessing in natural language processing.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "print(\"Original tokens:\", tokens)\n",
    "print(\"Filtered tokens without stop words:\", filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef53bcac",
   "metadata": {},
   "source": [
    "# 5.How does the process of removing punctuation contribute to text preprocessing in NLP? What are its benefits?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afdc08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Removing punctuation is an essential step in text preprocessing in natural language processing (NLP). \n",
    "Punctuation marks, such as periods, commas, and question marks, don't usually contribute much to the \n",
    "semantics of the text and can introduce noise or interfere with certain NLP tasks. The process of removing \n",
    "punctuation helps clean the text data and facilitates more effective analysis. \n",
    "\n",
    "Here are some benefits:\n",
    "\n",
    "1.Noise Reduction: \n",
    "    Punctuation marks often do not carry significant meaning in isolation. \n",
    "    Removing them reduces unnecessary noise and focuses on the actual content of the text.\n",
    "\n",
    "2.Consistent Tokenization: \n",
    "    Punctuation can affect the tokenization process. \n",
    "    Removing punctuation ensures a more consistent and reliable tokenization, as words are isolated without unwanted characters.\n",
    "\n",
    "3.Efficient Analysis: \n",
    "    Punctuation marks may not be relevant in many NLP tasks, such as sentiment analysis or topic modeling.\n",
    "    By removing them, the analysis can be more focused on meaningful words.\n",
    "\n",
    "4.Improved Model Performance: \n",
    "    In some cases, removing punctuation can lead to improved performance in machine learning models. \n",
    "    Punctuation marks may not provide useful features for certain tasks and excluding them can help \n",
    "    models concentrate on more informative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "349d406b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Removing punctuation is crucial for effective text preprocessing in NLP! It helps clean the data and facilitates analysis.\n",
      "Text without punctuation: Removing punctuation is crucial for effective text preprocessing in NLP It helps clean the data and facilitates analysis\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Sample text\n",
    "text = \"Removing punctuation is crucial for effective text preprocessing in NLP! It helps clean the data and facilitates analysis.\"\n",
    "\n",
    "# Remove punctuation\n",
    "clean_text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"Text without punctuation:\", clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb190ca",
   "metadata": {},
   "source": [
    "# 6. Discuss the importance of lowercase conversion in text preprocessing. Why is it a common step in NLP tasks?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82af1517",
   "metadata": {},
   "outputs": [],
   "source": [
    "Converting text to lowercase is a common and important step in text preprocessing for natural language processing (NLP) tasks.\n",
    "This process involves transforming all letters in the text to lowercase. \n",
    "Here are several reasons why lowercase conversion is often performed:\n",
    "\n",
    "1.Consistency in Text Matching:\n",
    "\n",
    "Converting text to lowercase ensures consistency in text matching. Without case consistency,\n",
    "words like \"Apple\" and \"apple\" would be treated as different, potentially leading to errors in analyses\n",
    "and tasks such as counting word frequencies.\n",
    "\n",
    "2.Standardization:\n",
    "\n",
    "Lowercasing helps standardize the text data. It ensures that all words are represented in a consistent format,\n",
    "making it easier to apply further processing steps consistently.\n",
    "\n",
    "3.Reduced Vocabulary Size:\n",
    "\n",
    "Lowercasing reduces the effective vocabulary size. Without lowercasing, words at the beginning of \n",
    "sentences (which are capitalized) and those within sentences would be treated as different tokens,\n",
    "increasing the complexity of the analysis.\n",
    "\n",
    "4.Improved Text Matching and Retrieval:\n",
    "\n",
    "Lowercasing is essential for tasks like information retrieval and search engines. \n",
    "When users enter queries, converting both the query and the document content to lowercase ensures \n",
    "that the search is case-insensitive.\n",
    "\n",
    "5.Efficient Tokenization:\n",
    "\n",
    "Lowercasing simplifies tokenization. When words are consistently in lowercase, tokenization becomes \n",
    "more straightforward as there is no need to account for different case variations.\n",
    "\n",
    "6.Improved Model Performance:\n",
    "\n",
    "In many NLP models, case differences might not contribute significantly to the meaning of the text. \n",
    "Lowercasing can help improve the performance of models by focusing on the semantic content of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70bec562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Converting text to lowercase is important in NLP tasks. It ensures consistency in text processing.\n",
      "Text in lowercase: converting text to lowercase is important in nlp tasks. it ensures consistency in text processing.\n"
     ]
    }
   ],
   "source": [
    "# Sample text\n",
    "text = \"Converting text to lowercase is important in NLP tasks. It ensures consistency in text processing.\"\n",
    "\n",
    "# Convert text to lowercase\n",
    "lowercased_text = text.lower()\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"Text in lowercase:\", lowercased_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d00762",
   "metadata": {},
   "source": [
    "# 7.Explain the term \"vectorization\" concerning text data. How does techniques like CountVectorizer contribute to text preprocessing in NLP?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41586e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorization in the context of text data refers to the process of converting textual data into \n",
    "numerical vectors that can be used as input for machine learning models. In natural language processing (NLP), \n",
    "vectorization is a crucial step, as most machine learning algorithms and models require numerical input. \n",
    "Vectorization methods transform words, phrases, or entire documents into numerical representations, allowing \n",
    "the algorithms to operate on the data.\n",
    "\n",
    "One common technique for vectorization is the CountVectorizer, which represents a document as a vector of word frequencies. \n",
    "It builds a vocabulary from all the words in the text and then counts the occurrences of each word for each document in\n",
    "the corpus.\n",
    "\n",
    "1.Word Frequency Representation:\n",
    "\n",
    "CountVectorizer converts each document in the corpus into a vector, where each element represents the \n",
    "frequency of a particular word in that document. This representation captures the distribution of words and \n",
    "their frequencies in the text.\n",
    "\n",
    "2.Sparse Matrix Representation:\n",
    "\n",
    "The result of CountVectorizer is often a sparse matrix, where most entries are zero. \n",
    "This sparse matrix efficiently represents the text data, saving memory and computational resources.\n",
    "\n",
    "3.Normalization:\n",
    "\n",
    "CountVectorizer can be configured to normalize the word frequencies, taking into account the length of the documents. \n",
    "This is useful for comparing documents of different lengths.\n",
    "\n",
    "4.Vocabulary Size Reduction:\n",
    "\n",
    "By setting parameters like maximum and minimum document frequency, CountVectorizer allows for the reduction\n",
    "of the vocabulary size. This can help remove very common or very rare words that may not contribute much to the analysis.\n",
    "\n",
    "5.Compatibility with Machine Learning Models:\n",
    "\n",
    "The numerical representation produced by CountVectorizer is compatible with a wide range of machine learning models, \n",
    "such as linear models, decision trees, and support vector machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e04d7980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "Vectorized matrix:\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample text data\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "# Create a CountVectorizer instance\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the text data\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names (words in the vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the sparse matrix to a dense array for better visualization\n",
    "dense_array = X.toarray()\n",
    "\n",
    "# Display the feature names and the resulting matrix\n",
    "print(\"Feature names:\", feature_names)\n",
    "print(\"Vectorized matrix:\")\n",
    "print(dense_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2048d2",
   "metadata": {},
   "source": [
    "# 8.Describe the concept of normalization in NLP. Provide examples of normalization techniques used in text preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da60600c",
   "metadata": {},
   "outputs": [],
   "source": [
    "In the context of natural language processing (NLP), normalization refers to the process of \n",
    "standardizing and transforming text data to a common format. The goal is to make the text consistent,\n",
    "reduce variations, and facilitate more meaningful comparisons. Normalization techniques are applied\n",
    "to ensure that similar words or phrases are represented in the same way, making it easier for \n",
    "algorithms to identify patterns and extract meaningful information from the text.\n",
    "\n",
    "Here are some common normalization techniques used in text preprocessing:\n",
    "\n",
    "1.Lowercasing:\n",
    "\n",
    "Converting all characters in the text to lowercase. This ensures case consistency and simplifies further processing.\n",
    "\n",
    "2.Stemming:\n",
    "\n",
    "Reducing words to their root or base form by removing prefixes or suffixes. For example, \"running\" becomes \"run.\"\n",
    "\n",
    "3.Lemmatization:\n",
    "\n",
    "Similar to stemming but involves reducing words to their base or dictionary form (lemma). For example, \"better\" becomes \"good.\"\n",
    "\n",
    "4.Removing Accents/Diacritics:\n",
    "\n",
    "Replacing accented characters with their non-accented counterparts. For example, converting \"résumé\" to \"resume.\"\n",
    "\n",
    "5.Removing Special Characters and Punctuation:\n",
    "\n",
    "Eliminating non-alphanumeric characters and punctuation marks from the text.\n",
    "\n",
    "6.Handling Numbers:\n",
    "\n",
    "Standardizing the representation of numbers. For example, converting \"3.14\" to \"3.1416\" or replacing \n",
    "numbers with a generic token like \"<NUM>\".\n",
    "\n",
    "7.Removing Stopwords:\n",
    "\n",
    "Eliminating common words that do not carry much meaning, such as \"the,\" \"and,\" \"is.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0b25cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Normalization is a crucial step in NLP. It involves converting text to lowercase, removing punctuation, and handling numbers.\n",
      "Normalized tokens: ['normalization', 'crucial', 'step', 'nlp', '', 'involves', 'converting', 'text', 'lowercase', '', 'removing', 'punctuation', '', 'handling', 'number', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Sample text\n",
    "text = \"Normalization is a crucial step in NLP. It involves converting text to lowercase, removing punctuation, and handling numbers.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Lowercasing\n",
    "lowercased_tokens = [token.lower() for token in tokens]\n",
    "\n",
    "# Removing punctuation and special characters\n",
    "cleaned_tokens = [re.sub(r'[^\\w\\s]', '', token) for token in lowercased_tokens]\n",
    "\n",
    "# Removing stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in cleaned_tokens if token not in stop_words]\n",
    "\n",
    "# Stemming\n",
    "porter_stemmer = PorterStemmer()\n",
    "stemmed_tokens = [porter_stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "# Display the results\n",
    "print(\"Original text:\", text)\n",
    "print(\"Normalized tokens:\", lemmatized_tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
