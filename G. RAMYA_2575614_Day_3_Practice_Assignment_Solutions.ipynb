{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b1feeb3",
   "metadata": {},
   "source": [
    "# Fraud Detection with Logistic Regression and Feature Engineering\n",
    "\n",
    "Day 3\n",
    "\n",
    "You are a data scientist at a financial institution, and your primary task is to develop a fraud detection model using logistic regression. The dataset you have is highly imbalanced, with only a small fraction of transactions being fraudulent. Your objective is to create an effective model by implementing logistic regression and employing various feature engineering techniques to improve the model's performance:\n",
    "\n",
    "1. Data Preparation:\n",
    "\n",
    "a. Load the dataset, and provide an overview of the available features, including transaction details, customer information, and labels (fraudulent or non-fraudulent).\n",
    "\n",
    "b. Describe the class distribution of fraudulent and non-fraudulent transactions and discuss the imbalance issue.\n",
    "\n",
    "2. Initial Logistic Regression Model:\n",
    "\n",
    "a. Implement a basic logistic regression model using the raw dataset.\n",
    "\n",
    "b. Evaluate the model's performance using standard metrics like accuracy, precision, recall, and F1-score.\n",
    "\n",
    "3. Feature Engineering:\n",
    "\n",
    "a. Apply feature engineering techniques to enhance the predictive power of the model. These techniques may include:\n",
    "\n",
    "-Creating new features.\n",
    "\n",
    "- Scaling or normalizing features.\n",
    "\n",
    "Handling missing values.\n",
    "\n",
    "-Encoding categorical variables.\n",
    "\n",
    "b. Explain why each feature engineering technique is relevant for fraud detection.\n",
    "\n",
    "4. Handling Imbalanced Data:\n",
    "\n",
    "a. Discuss the challenges associated with imbalanced datasets in the context of fraud detection.\n",
    "\n",
    "b. Implement strategies to address class imbalance, such as:\n",
    "\n",
    "-Oversampling the minority class.\n",
    "\n",
    "Undersampling the majority class.\n",
    "\n",
    "Using synthetic data generation techniques (e.g., SMOTE).\n",
    "\n",
    "5. Logistic Regression with Feature-Engineered Data:\n",
    "\n",
    "a. Train a logistic regression model using the feature-engineered dataset and the methods for handling imbalanced data.\n",
    "\n",
    "b. Evaluate the model's performance using appropriate evaluation metrics:\n",
    "\n",
    "6. Model Interpretation:\n",
    "\n",
    "a. Interpret the coefficients of the logistic regression model and discuss which features have the most influence on fraud detection.\n",
    "\n",
    "b. Explain how the logistic regression model can be used for decision-making in identifying potential fraud.\n",
    "\n",
    "7. Model Comparison:\n",
    "\n",
    "a. Compare the performance of the initial logistic regression model with the feature-engineered and balanced data model.\n",
    "\n",
    "b. Discuss the advantages and limitations of each approach.\n",
    "\n",
    "8. Presentation and Recommendations:\n",
    "\n",
    "a. Prepare a presentation or report summarizing your analysis, results, and recommendations for the financial institution. Highlight the importance of feature engineering and handling imbalanced data in building an effective fraud detection system.\n",
    "\n",
    "In this case study, you are required to showcase your ability to preprocess data, implement logistic regression, apply feature engineering techniques, and address class imbalance to improve the model's performance. Your analysis should also demonstrate your understanding of the nuances of fraud detection in a financial context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e9e808",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9eeebb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
      "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
      "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
      "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4  0.502292  0.219422  0.215153   69.99      0  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset (replace 'dataset.csv' with the actual file path)\n",
    "data = pd.read_csv(\"C://Users//TmC//Downloads//archive//creditcard.csv\")\n",
    "\n",
    "# Display the first few rows of the dataset to get an overview of the features\n",
    "print(data.head())\n",
    "\n",
    "# Check the column names and data types\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f04eba2",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "670e3cdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance Metrics:\n",
      "Accuracy: 1.00\n",
      "Precision: 0.62\n",
      "Recall: 0.55\n",
      "F1 Score: 0.58\n",
      "\n",
      "Confusion Matrix:\n",
      "[[56831    33]\n",
      " [   44    54]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TmC\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Load the dataset (replace 'creditcard.csv' with the actual file path)\n",
    "data = pd.read_csv('creditcard.csv')\n",
    "\n",
    "# Define your features (independent variables) and target (dependent variable)\n",
    "X = data.drop('Class', axis=1)  # Features\n",
    "y = data['Class']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Model Performance Metrics:\")\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1 Score: {:.2f}\".format(f1))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e185115f",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "004d16ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "49637730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6cc04843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the scaler on the training data\n",
    "X_train['Amount'] = scaler.fit_transform(X_train[['Amount']])\n",
    "\n",
    "# Transform the test data using the fitted scaler\n",
    "X_test['Amount'] = scaler.transform(X_test[['Amount']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b3ecb7",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5e6b5783",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance Metrics:\n",
      "Accuracy: 1.00\n",
      "Precision: 0.62\n",
      "Recall: 0.55\n",
      "F1 Score: 0.58\n",
      "\n",
      "Confusion Matrix:\n",
      "[[56831    33]\n",
      " [   44    54]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.62      0.55      0.58        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.81      0.78      0.79     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TmC\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Load the feature-engineered dataset\n",
    "data = pd.read_csv(\"C://Users//TmC//Downloads//archive//creditcard.csv\")  # Replace with your actual file path\n",
    "\n",
    "# Define your features (independent variables) and target (dependent variable)\n",
    "X = data.drop('Class', axis=1)  # Features\n",
    "y = data['Class']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Model Performance Metrics:\")\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1 Score: {:.2f}\".format(f1))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940b1703",
   "metadata": {},
   "source": [
    "# Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e4ade89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature  Coefficient  Abs_Coefficient\n",
      "14     V14    -1.081815         1.081815\n",
      "2       V2    -0.880399         0.880399\n",
      "17     V17    -0.746935         0.746935\n",
      "3       V3    -0.733691         0.733691\n",
      "9       V9    -0.558533         0.558533\n",
      "1       V1     0.533949         0.533949\n",
      "8       V8    -0.509440         0.509440\n",
      "7       V7     0.431350         0.431350\n",
      "15     V15    -0.430041         0.430041\n",
      "13     V13    -0.428819         0.428819\n",
      "16     V16    -0.419183         0.419183\n",
      "22     V22     0.391334         0.391334\n",
      "25     V25    -0.386550         0.386550\n",
      "10     V10    -0.336703         0.336703\n",
      "5       V5     0.286533         0.286533\n",
      "21     V21     0.285225         0.285225\n",
      "11     V11    -0.231455         0.231455\n",
      "4       V4     0.200136         0.200136\n",
      "6       V6    -0.103810         0.103810\n",
      "27     V27    -0.094108         0.094108\n",
      "28     V28     0.071549         0.071549\n",
      "20     V20     0.066653         0.066653\n",
      "19     V19     0.056360         0.056360\n",
      "26     V26     0.054270         0.054270\n",
      "23     V23     0.050683         0.050683\n",
      "18     V18    -0.036417         0.036417\n",
      "12     V12    -0.027380         0.027380\n",
      "24     V24    -0.022557         0.022557\n",
      "29  Amount    -0.010464         0.010464\n",
      "0     Time    -0.000080         0.000080\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have already fitted a logistic regression model (model) and have the feature names\n",
    "coefficients = model.coef_[0]\n",
    "feature_names = X.columns\n",
    "\n",
    "# Create a dataframe to display the feature names and their corresponding coefficients\n",
    "coeff_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
    "\n",
    "# Sort the dataframe by the absolute magnitude of the coefficients\n",
    "coeff_df['Abs_Coefficient'] = abs(coeff_df['Coefficient'])\n",
    "sorted_coeff_df = coeff_df.sort_values(by='Abs_Coefficient', ascending=False)\n",
    "\n",
    "# Display the sorted coefficients\n",
    "print(sorted_coeff_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87a457e",
   "metadata": {},
   "source": [
    "# Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad0e4216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TmC\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\TmC\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Model Performance:\n",
      "Accuracy: 1.00\n",
      "Precision: 0.66\n",
      "Recall: 0.56\n",
      "F1 Score: 0.61\n",
      "AUC-ROC: 0.90\n",
      "\n",
      "Feature-Engineered Model Performance:\n",
      "Accuracy: 1.00\n",
      "Precision: 0.66\n",
      "Recall: 0.56\n",
      "F1 Score: 0.61\n",
      "AUC-ROC: 0.90\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Define and train the initial logistic regression model\n",
    "model_initial = LogisticRegression()\n",
    "model_initial.fit(X_train, y_train)\n",
    "\n",
    "# Define and train the feature-engineered logistic regression model\n",
    "model_feature_engineered = LogisticRegression()\n",
    "model_feature_engineered.fit(X_train, y_train)  # Use the same X_train and y_train for feature-engineered model\n",
    "\n",
    "# Initial Logistic Regression Model\n",
    "y_pred_initial = model_initial.predict(X_test)\n",
    "\n",
    "# Performance metrics for the initial model\n",
    "accuracy_initial = accuracy_score(y_test, y_pred_initial)\n",
    "precision_initial = precision_score(y_test, y_pred_initial)\n",
    "recall_initial = recall_score(y_test, y_pred_initial)\n",
    "f1_initial = f1_score(y_test, y_pred_initial)\n",
    "roc_auc_initial = roc_auc_score(y_test, model_initial.predict_proba(X_test)[:,1])\n",
    "\n",
    "# Feature-Engineered and Balanced Data Model\n",
    "y_pred_feature_engineered = model_feature_engineered.predict(X_test)\n",
    "\n",
    "# Performance metrics for the feature-engineered model\n",
    "accuracy_feature_engineered = accuracy_score(y_test, y_pred_feature_engineered)\n",
    "precision_feature_engineered = precision_score(y_test, y_pred_feature_engineered)\n",
    "recall_feature_engineered = recall_score(y_test, y_pred_feature_engineered)\n",
    "f1_feature_engineered = f1_score(y_test, y_pred_feature_engineered)\n",
    "roc_auc_feature_engineered = roc_auc_score(y_test, model_feature_engineered.predict_proba(X_test)[:,1])\n",
    "\n",
    "# Compare the performance metrics\n",
    "print(\"Initial Model Performance:\")\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_initial))\n",
    "print(\"Precision: {:.2f}\".format(precision_initial))\n",
    "print(\"Recall: {:.2f}\".format(recall_initial))\n",
    "print(\"F1 Score: {:.2f}\".format(f1_initial))\n",
    "print(\"AUC-ROC: {:.2f}\".format(roc_auc_initial))\n",
    "\n",
    "print(\"\\nFeature-Engineered Model Performance:\")\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_feature_engineered))\n",
    "print(\"Precision: {:.2f}\".format(precision_feature_engineered))\n",
    "print(\"Recall: {:.2f}\".format(recall_feature_engineered))\n",
    "print(\"F1 Score: {:.2f}\".format(f1_feature_engineered))\n",
    "print(\"AUC-ROC: {:.2f}\".format(roc_auc_feature_engineered))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc94d007",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f1724b5",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load your dataset (replace 'creditcard.csv' with your dataset)\n",
    "data = pd.read_csv(\"C://Users//TmC//Downloads//archive//creditcard.csv\")\n",
    "\n",
    "# Define your features (independent variables) and target (dependent variable)\n",
    "X = data.drop('Class', axis=1)  # Features\n",
    "y = data['Class']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Choose the class imbalance strategy\n",
    "\n",
    "# 1. Oversampling the Minority Class using SMOTE\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# 2. Undersampling the Majority Class\n",
    "undersampler = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "X_train_undersampled, y_train_undersampled = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# 3. Using a combination of SMOTE and Tomek links\n",
    "smote_tomek = SMOTETomek(sampling_strategy='auto', random_state=42)\n",
    "X_train_smote_tomek, y_train_smote_tomek = smote_tomek.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train a logistic regression model on each of the balanced datasets\n",
    "model_oversampled = LogisticRegression()\n",
    "model_oversampled.fit(X_train_oversampled, y_train_oversampled)\n",
    "\n",
    "model_undersampled = LogisticRegression()\n",
    "model_undersampled.fit(X_train_undersampled, y_train_undersampled)\n",
    "\n",
    "model_smote_tomek = LogisticRegression()\n",
    "model_smote_tomek.fit(X_train_smote_tomek, y_train_smote_tomek)\n",
    "\n",
    "# Evaluate the models on the test set\n",
    "y_pred_oversampled = model_oversampled.predict(X_test)\n",
    "y_pred_undersampled = model_undersampled.predict(X_test)\n",
    "y_pred_smote_tomek = model_smote_tomek.predict(X_test)\n",
    "\n",
    "# Now, you can evaluate the performance of these models using various metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
