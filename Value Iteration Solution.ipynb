{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "from lib.envs.gridworld import GridworldEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.0001, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V) of the optimal policy and the optimal value function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.nA containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            for prob, next_state, reward, done in env.P[state][a]:\n",
    "                A[a] += prob * (reward + discount_factor * V[next_state])\n",
    "        return A\n",
    "    \n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        # Stopping condition\n",
    "        delta = 0\n",
    "        # Update each state...\n",
    "        for s in range(env.nS):\n",
    "            # Do a one-step lookahead to find the best action\n",
    "            A = one_step_lookahead(s, V)\n",
    "            best_action_value = np.max(A)\n",
    "            # Calculate delta across all states seen so far\n",
    "            delta = max(delta, np.abs(best_action_value - V[s]))\n",
    "            # Update the value function. Ref: Sutton book eq. 4.10. \n",
    "            V[s] = best_action_value        \n",
    "        # Check if we can stop \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # Create a deterministic policy using the optimal value function\n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    for s in range(env.nS):\n",
    "        # One step lookahead to find the best action for this state\n",
    "        A = one_step_lookahead(s, V)\n",
    "        best_action = np.argmax(A)\n",
    "        # Always take the best action\n",
    "        policy[s, best_action] = 1.0\n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Probability Distribution:\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n",
      "Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\n",
      "[[0 3 3 2]\n",
      " [0 0 0 2]\n",
      " [0 0 1 2]\n",
      " [0 1 1 0]]\n",
      "\n",
      "Value Function:\n",
      "[ 0. -1. -2. -3. -1. -2. -3. -2. -2. -3. -2. -1. -3. -2. -1.  0.]\n",
      "\n",
      "Reshaped Grid Value Function:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy, v = value_iteration(env)\n",
    "\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy:\n",
      "[[0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]]\n",
      "Optimal Value Function:\n",
      "[944.72357234 864.01312811 903.55729147 873.75063348 789.53799752\n",
      " 864.01312811 789.53799565 816.76688902 864.01312999 826.02716258\n",
      " 903.55729147 835.38097132 807.59922297 826.02716258 807.59922109\n",
      " 873.75063348 955.27633662 873.75063542 913.69423476 883.58649945\n",
      " 934.27633662 854.37299683 893.52171855 864.01312715 798.52323074\n",
      " 873.75063542 798.52322888 826.02716161 854.37299869 816.76689095\n",
      " 893.52171855 826.02716161 816.76689281 835.38097326 816.76689095\n",
      " 883.58649945 944.72357326 883.58650137 903.55729242 893.5217176\n",
      " 883.58650321 807.59922204 844.82926686 816.76688999 844.8292687\n",
      " 923.93357142 844.82926686 873.75063446 844.8292687  807.59922204\n",
      " 883.58650137 816.76688999 826.02716536 844.82926686 826.02716353\n",
      " 893.5217176  893.52172131 934.2763357  893.52171949 903.55729147\n",
      " 873.75063818 798.52322982 835.3809742  807.59922109 854.37299959\n",
      " 934.2763357  854.37299777 883.58650042 835.38097602 798.52322982\n",
      " 873.75063635 807.59922109 835.38097602 854.37299777 835.3809742\n",
      " 903.55729147 883.5865041  944.72357234 883.5865023  913.69423476\n",
      " 864.01313179 789.53799752 826.02716445 798.52322888 864.01313179\n",
      " 944.72357234 864.01312999 893.52171855 826.02716626 789.53799752\n",
      " 864.01312999 798.52322888 826.02716626 844.82926779 826.02716445\n",
      " 893.52171855 873.75063906 955.27633662 873.75063727 903.55729242\n",
      " 934.27633662 854.37299683 893.52171855 864.01312715 798.52323074\n",
      " 873.75063542 798.52322888 826.02716161 873.75063727 835.38097326\n",
      " 913.69423476 844.82926495 816.76689281 835.38097326 816.76689095\n",
      " 883.58649945 944.72357326 883.58650137 923.93357142 893.5217176\n",
      " 923.93357326 844.82926686 883.58650137 854.37299588 807.59922388\n",
      " 883.58650137 807.59922204 835.3809723  864.0131309  826.02716353\n",
      " 903.55729242 835.3809723  826.02716536 844.82926686 826.02716353\n",
      " 893.5217176  934.27633752 893.52171949 913.6942357  903.55729147\n",
      " 893.52172131 816.76689189 854.37299777 826.02716258 835.38097602\n",
      " 913.6942357  835.3809742  864.01312811 854.37299959 816.76689189\n",
      " 893.52171949 826.02716258 835.38097602 854.37299777 835.3809742\n",
      " 903.55729147 903.55729515 923.93357234 903.55729334 913.69423476\n",
      " 883.5865041  807.59922297 844.82926779 816.76689095 844.8292696\n",
      " 923.93357234 844.82926779 873.75063542 844.8292696  807.59922297\n",
      " 883.5865023  816.76689095 844.8292696  864.01312999 844.82926779\n",
      " 913.69423476 893.5217222  934.27633662 893.52172041 923.93357142\n",
      " 873.75063906 798.52323074 835.38097512 807.59922204 854.37300048\n",
      " 934.27633662 854.37299869 883.58650137 835.3809769  798.52323074\n",
      " 873.75063727 807.59922204 835.3809769  854.37299869 835.38097512\n",
      " 903.55729242 883.58650497 944.72357326 883.58650321 913.6942357\n",
      " 923.93357326 844.82926686 883.58650137 854.37299588 807.59922388\n",
      " 883.58650137 807.59922204 835.3809723  883.58650321 844.82926686\n",
      " 923.93357142 854.37299588 826.02716536 844.82926686 826.02716353\n",
      " 893.5217176  934.27633752 893.52171949 934.2763357  903.55729147\n",
      " 913.69423752 835.3809742  873.75063635 844.82926592 816.76689371\n",
      " 893.52171949 816.76689189 844.82926592 873.75063818 835.3809742\n",
      " 913.6942357  844.82926592 835.38097602 854.37299777 835.3809742\n",
      " 903.55729147 923.93357415 903.55729334 923.93357234 913.69423476\n",
      " 903.55729515 826.02716445 864.01312999 835.38097326 826.02716626\n",
      " 903.55729334 826.02716445 854.37299683 864.01313179 826.02716445\n",
      " 903.55729334 835.38097326 844.8292696  864.01312999 844.82926779\n",
      " 913.69423476 913.69423841 913.69423662 913.69423662 923.93357142\n",
      " 893.5217222  816.76689281 854.37299869 826.02716353 835.3809769\n",
      " 913.69423662 835.38097512 864.01312906 854.37300048 816.76689281\n",
      " 893.52172041 826.02716353 854.37300048 873.75063727 854.37299869\n",
      " 923.93357142 903.55729602 923.93357326 903.55729426 934.2763357\n",
      " 883.58650497 807.59922388 844.8292687  816.76689189 844.82927047\n",
      " 923.93357326 844.8292687  873.75063635 844.82927047 807.59922388\n",
      " 883.58650321 816.76689189 844.82927047 864.0131309  844.8292687\n",
      " 913.6942357  893.52172306 934.27633752 893.52172131 923.93357234\n",
      " 913.69423752 835.3809742  873.75063635 844.82926592 798.52323164\n",
      " 873.75063635 798.52322982 826.02716258 893.52172131 854.37299777\n",
      " 934.2763357  864.01312811 816.76689371 835.3809742  816.76689189\n",
      " 883.58650042 923.93357415 883.5865023  944.72357234 893.52171855\n",
      " 903.55729515 826.02716445 864.01312999 835.38097326 807.59922477\n",
      " 883.5865023  807.59922297 835.38097326 864.01313179 826.02716445\n",
      " 903.55729334 835.38097326 826.02716626 844.82926779 826.02716445\n",
      " 893.52171855 913.69423841 893.52172041 913.69423662 903.55729242\n",
      " 893.5217222  816.76689281 854.37299869 826.02716353 816.76689459\n",
      " 893.52172041 816.76689281 844.82926686 854.37300048 816.76689281\n",
      " 893.52172041 826.02716353 835.3809769  854.37299869 835.38097512\n",
      " 903.55729242 903.55729602 903.55729426 903.55729426 913.6942357\n",
      " 883.58650497 807.59922388 844.8292687  816.76689189 826.02716713\n",
      " 903.55729426 826.02716536 854.37299777 844.82927047 807.59922388\n",
      " 883.58650321 816.76689189 864.01313267 883.58650321 864.0131309\n",
      " 934.2763357  893.52172306 913.69423752 893.52172131 944.72357234\n",
      " 873.75063992 798.52323164 835.38097602 807.59922297 835.38097777\n",
      " 913.69423752 835.38097602 864.01312999 835.38097777 798.52323164\n",
      " 873.75063818 807.59922297 854.37300134 873.75063818 854.37299959\n",
      " 923.93357234 883.58650583 923.93357415 883.5865041  934.27633662\n",
      " 903.55729515 826.02716445 864.01312999 835.38097326 789.53799933\n",
      " 864.01312999 789.53799752 816.76689095 903.55729515 864.01312999\n",
      " 944.72357234 873.75063542 807.59922477 826.02716445 807.59922297\n",
      " 873.75063542 913.69423841 873.75063727 955.27633662 883.58650137\n",
      " 893.5217222  816.76689281 854.37299869 826.02716353 798.52323253\n",
      " 873.75063727 798.52323074 826.02716353 854.37300048 816.76689281\n",
      " 893.52172041 826.02716353 816.76689459 835.38097512 816.76689281\n",
      " 883.58650137 903.55729602 883.58650321 903.55729426 893.52171949\n",
      " 883.58650497 807.59922388 844.8292687  816.76689189 807.59922565\n",
      " 883.58650321 807.59922388 835.3809742  844.82927047 807.59922388\n",
      " 883.58650321 816.76689189 826.02716713 844.8292687  826.02716536\n",
      " 893.52171949 893.52172306 893.52172131 893.52172131 903.55729334\n",
      " 873.75063992 798.52323164 835.38097602 807.59922297 816.76689546\n",
      " 893.52172131 816.76689371 844.82926779 835.38097777 798.52323164\n",
      " 873.75063818 807.59922297 873.75063992 893.52172131 873.75063818\n",
      " 944.72357234 883.58650583 903.55729515 883.5865041  955.27633662\n",
      " 864.01313352 789.53799933 826.02716626 798.52323074 826.02716799\n",
      " 903.55729515 826.02716626 854.37299869 826.02716799 789.53799933\n",
      " 864.01313179 798.52323074 864.01313352 883.5865041  864.01313179\n",
      " 934.27633662 873.75064077 913.69423841 873.75063906 944.72357326]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "def value_iteration(env, gamma=0.99, theta=1e-6):\n",
    "    nS = env.observation_space.n  # Number of states\n",
    "    nA = env.action_space.n  # Number of actions\n",
    "\n",
    "    V = np.zeros(nS)  # Initialize the value function with zeros\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(nS):\n",
    "            v = V[s]\n",
    "            action_values = [sum(p * (r + gamma * V[s_next]) for p, s_next, r, _ in env.P[s][a]) for a in range(nA)]\n",
    "            V[s] = max(action_values)\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    # Extract the optimal policy based on the optimal value function\n",
    "    policy = np.zeros([nS, nA])\n",
    "    for s in range(nS):\n",
    "        action_values = [sum(p * (r + gamma * V[s_next]) for p, s_next, r, _ in env.P[s][a]) for a in range(nA)]\n",
    "        best_action = np.argmax(action_values)\n",
    "        policy[s][best_action] = 1\n",
    "\n",
    "    return policy, V\n",
    "\n",
    "# Create the Taxi-v3 environment\n",
    "env = gym.make('Taxi-v3')\n",
    "\n",
    "# Run the Value Iteration algorithm to obtain the optimal policy and value function\n",
    "optimal_policy, optimal_value = value_iteration(env)\n",
    "\n",
    "# Print the optimal policy and value function\n",
    "print(\"Optimal Policy:\")\n",
    "print(optimal_policy)\n",
    "\n",
    "print(\"Optimal Value Function:\")\n",
    "print(optimal_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
