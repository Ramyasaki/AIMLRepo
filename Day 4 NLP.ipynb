{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fc43624",
   "metadata": {},
   "source": [
    "# Create Your Own Spell Checker\n",
    "Objective: Creating a spell checker, correct the incorrect word in the given sentence.\n",
    "Problem Statement: While typing or sending any message to person, we generally make \n",
    "spelling mistakes. Write a script which will correct the misspelled words in a sentence. \n",
    "The input will be a raw string and the output will be a string with the case normalized \n",
    "and the incorrect word corrected.\n",
    "Domain: General\n",
    "Analysis to be done: Words availability in corpus\n",
    "Content: \n",
    "Dataset: None\n",
    "We will be using NLTK’s inbuilt corpora (words, stop words etc.) and no specific dataset.\n",
    "Steps to perform:\n",
    "While there are several approaches to correct spelling , you will use the Levenshtein or \n",
    "Edit distance approach. \n",
    "The approach will be straightforward for correcting a word: \n",
    "▪ If the word is present in a list of valid words, the word is correct.\n",
    "▪ If the word is absent from the valid word list, we will find the correct \n",
    "word, i.e., the word from the valid word list which has the lowest edit \n",
    "distance from the target word.\n",
    "Once you define a function, you will iterate over the terms in the given sentence, \n",
    "correct the words identified as incorrect, and return a joined string with all the terms. \n",
    "To help speed up execution, you won’t be applying the spell check on the stop words\n",
    "and punctuation\n",
    "\n",
    "Tasks: \n",
    "1. Get a list of valid words in the English language using NLTK’s list of words (Hint: \n",
    "use nltk.download(‘words’) to get the raw list.\n",
    "2. Look at the first 20 words in the list. Is the casing normalized?\n",
    "3. Normalize the casing for all the terms.\n",
    "4. Some duplicates would have been induced, create unique list after normalizing.\n",
    "5. Create a list of stop words which should include: \n",
    "i. Stop words from NLTK\n",
    "ii. All punctuations (Hint: use ‘punctuation’ from string module)\n",
    "iii. Final list should be a combination of these two\n",
    "6. Define a function to get correct a single term\n",
    "• For a given term, find its edit distance with each term in the valid word \n",
    "list. To speed up execution, you can use the first 20,000 entries in the \n",
    "valid word list.\n",
    "• Store the result in a dictionary, the key as the term, and edit distance as \n",
    "value.\n",
    "• Sort the dictionary in ascending order of the values.\n",
    "• Return the first entry in the sorted result (value with minimum edit \n",
    "distance).\n",
    "• Using the function, get the correct word for committee.\n",
    "7. Make a set from the list of valid words, for faster lookup to see if word is in valid \n",
    "list or not.\n",
    "8. Define a function for spelling correction in any given input sentence:\n",
    "1. To tokenize them after making all the terms in lowercase \n",
    "For each term in the tokenized sentence:\n",
    "2. Check if the term is in the list of valid words (valid_words_set).\n",
    "3. If yes, return the word as is.\n",
    "4. If no, get the correct word using get_correct_term function.\n",
    "5. To return the joined string as output.\n",
    "9. Test the function for the input sentence “The new abacos is great”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61c04306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\tmc\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\tmc\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\tmc\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\tmc\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tmc\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tmc\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\TmC\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\TmC\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "240e115d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\TmC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\TmC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet2022 to\n",
      "[nltk_data]     C:\\Users\\TmC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet2022 is already up-to-date!\n",
      "'cp' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "from nltk import wsd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from spacy.cli import download\n",
    "from spacy import load\n",
    "import warnings\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('wordnet2022')\n",
    "\n",
    "\n",
    "! cp -rf /usr/share/nltk_data/corpora/wordnet2022 /usr/share/nltk_data/corpora/wordnet # temp fix for lookup error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce81506",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8454a919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 words in the list: ['A', 'a', 'aa', 'aal', 'aalii', 'aam', 'Aani', 'aardvark', 'aardwolf', 'Aaron', 'Aaronic', 'Aaronical', 'Aaronite', 'Aaronitic', 'Aaru', 'Ab', 'aba', 'Ababdeh', 'Ababua', 'abac']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\TmC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the words corpus if not already downloaded\n",
    "nltk.download('words')\n",
    "\n",
    "# Get the list of valid English words\n",
    "valid_words = nltk.corpus.words.words()\n",
    "\n",
    "# Print the first 20 words in the list\n",
    "print(\"First 20 words in the list:\", valid_words[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91087fa3",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98cff6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\TmC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "a\n",
      "aa\n",
      "aal\n",
      "aalii\n",
      "aam\n",
      "aani\n",
      "aardvark\n",
      "aardwolf\n",
      "aaron\n",
      "aaronic\n",
      "aaronical\n",
      "aaronite\n",
      "aaronitic\n",
      "aaru\n",
      "ab\n",
      "aba\n",
      "ababdeh\n",
      "ababua\n",
      "abac\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the words corpus if not already downloaded\n",
    "nltk.download('words')\n",
    "\n",
    "# Get the list of valid English words and normalize the casing\n",
    "valid_words = [word.lower() for word in nltk.corpus.words.words()]\n",
    "\n",
    "# Print the first 20 words in the list with normalized casing\n",
    "for word in valid_words[:20]:\n",
    "    print(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366e6b77",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58fe8c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\TmC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 words in the list with normalized casing: ['a', 'a', 'aa', 'aal', 'aalii', 'aam', 'aani', 'aardvark', 'aardwolf', 'aaron', 'aaronic', 'aaronical', 'aaronite', 'aaronitic', 'aaru', 'ab', 'aba', 'ababdeh', 'ababua', 'abac']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the words corpus if not already downloaded\n",
    "nltk.download('words')\n",
    "\n",
    "# Get the list of valid English words and normalize the casing\n",
    "valid_words = [word.lower() for word in nltk.corpus.words.words()]\n",
    "\n",
    "# Print the first 20 words in the list with normalized casing\n",
    "for word in valid_words[:20]:\n",
    "    print(word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5f9b35",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "255fc0d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\TmC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 words in the unique list with normalized casing: ['tonometry', 'dishonorable', 'anklejack', 'phaenogamia', 'nivellation', 'bidential', 'pachyvaginitis', 'stagnum', 'letteret', 'lepidosaurian', 'inedibility', 'placentalian', 'kuroshio', 'flatfish', 'bridebowl', 'taranchi', 'cataloguer', 'latrine', 'inferrer', 'aphakial']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the words corpus if not already downloaded\n",
    "nltk.download('words')\n",
    "\n",
    "# Get the list of valid English words and normalize the casing\n",
    "valid_words = [word.lower() for word in nltk.corpus.words.words()]\n",
    "\n",
    "# Create a unique list after normalizing casing\n",
    "unique_valid_words = list(set(valid_words))\n",
    "\n",
    "# Print the first 20 words in the unique list\n",
    "print(\"First 20 words in the unique list with normalized casing:\", unique_valid_words[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4ee604",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dfa3b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 words in the final list of stop words: ['too', 'your', 'were', 'll', 't', \"aren't\", 'during', 'up', 'them', 'other', \"won't\", '<', 'over', \"it's\", 'don', ':', 'him', \"hasn't\", 'against', 'each']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TmC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "# Download the NLTK stop words if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the list of stop words from NLTK\n",
    "stop_words_nltk = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Get all punctuations from the string module\n",
    "punctuations = set(string.punctuation)\n",
    "\n",
    "# Create the final list of stop words by combining the two sets\n",
    "stop_words_final = stop_words_nltk.union(punctuations)\n",
    "\n",
    "# Print the first 20 words in the final list\n",
    "print(\"First 20 words in the final list of stop words:\", list(stop_words_final)[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e8b314",
   "metadata": {},
   "source": [
    "# 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ef87210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\TmC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct word for 'committee': commitment\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "# Download the words corpus if not already downloaded\n",
    "nltk.download('words')\n",
    "\n",
    "def get_correct_term(target_term, valid_words_list):\n",
    "    # Use the first 20,000 entries in the valid word list\n",
    "    valid_words_list = valid_words_list[:20000]\n",
    "\n",
    "    # Store the edit distances in a dictionary\n",
    "    edit_distances = {word: edit_distance(target_term, word) for word in valid_words_list}\n",
    "\n",
    "    # Sort the dictionary in ascending order of edit distances\n",
    "    sorted_distances = sorted(edit_distances.items(), key=lambda x: x[1])\n",
    "\n",
    "    # Return the first entry in the sorted result (minimum edit distance)\n",
    "    return sorted_distances[0][0]\n",
    "\n",
    "# Example: Get the correct word for 'committee'\n",
    "correct_word_committee = get_correct_term(\"committee\", valid_words_list)\n",
    "print(\"Correct word for 'committee':\", correct_word_committee)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02e6025",
   "metadata": {},
   "source": [
    "# 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da378692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\TmC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'example' is in the valid words list.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the words corpus if not already downloaded\n",
    "nltk.download('words')\n",
    "\n",
    "# Get the list of valid English words and normalize the casing\n",
    "valid_words_list = [word.lower() for word in nltk.corpus.words.words()]\n",
    "\n",
    "# Create a set from the list of valid words for faster lookup\n",
    "valid_words_set = set(valid_words_list)\n",
    "\n",
    "# Example: Check if a word is in the valid words set\n",
    "word_to_check = 'example'\n",
    "if word_to_check in valid_words_set:\n",
    "    print(f\"'{word_to_check}' is in the valid words list.\")\n",
    "else:\n",
    "    print(f\"'{word_to_check}' is not in the valid words list.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf07157",
   "metadata": {},
   "source": [
    "# 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e46b2d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\TmC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Sentence: the new abatis is great\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "# Download the words corpus if not already downloaded\n",
    "nltk.download('words')\n",
    "\n",
    "def get_valid_words_set():\n",
    "    # Get the list of valid English words and normalize the casing\n",
    "    valid_words_list = [word.lower() for word in nltk.corpus.words.words()]\n",
    "    # Create a set from the list of valid words for faster lookup\n",
    "    return set(valid_words_list)\n",
    "\n",
    "def get_correct_term(target_term, valid_words_list):\n",
    "    # Use the first 20,000 entries in the valid word list\n",
    "    valid_words_list = valid_words_list[:20000]\n",
    "\n",
    "    # Store the edit distances in a dictionary\n",
    "    edit_distances = {word: edit_distance(target_term, word) for word in valid_words_list}\n",
    "\n",
    "    # Sort the dictionary in ascending order of edit distances\n",
    "    sorted_distances = sorted(edit_distances.items(), key=lambda x: x[1])\n",
    "\n",
    "    # Return the first entry in the sorted result (minimum edit distance)\n",
    "    return sorted_distances[0][0]\n",
    "\n",
    "def correct_spelling(input_sentence, valid_words_set):\n",
    "    # Tokenize the input sentence after making all terms lowercase\n",
    "    tokenized_sentence = nltk.word_tokenize(input_sentence.lower())\n",
    "\n",
    "    # Correct spelling for each term in the tokenized sentence\n",
    "    corrected_sentence = [term if term in valid_words_set else get_correct_term(term, list(valid_words_set)) for term in tokenized_sentence]\n",
    "\n",
    "    # Return the joined string as output\n",
    "    return ' '.join(corrected_sentence)\n",
    "\n",
    "# Example usage:\n",
    "input_sentence = \"The new abacos is great\"\n",
    "valid_words_set = get_valid_words_set()\n",
    "corrected_sentence = correct_spelling(input_sentence, valid_words_set)\n",
    "print(\"Corrected Sentence:\", corrected_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569d43d3",
   "metadata": {},
   "source": [
    "# 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3520afe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\TmC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: The new abacos is great\n",
      "Corrected Sentence: the new abatis is great\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "# Download the words corpus if not already downloaded\n",
    "nltk.download('words')\n",
    "\n",
    "def get_valid_words_set():\n",
    "    # Get the list of valid English words and normalize the casing\n",
    "    valid_words_list = [word.lower() for word in nltk.corpus.words.words()]\n",
    "    # Create a set from the list of valid words for faster lookup\n",
    "    return set(valid_words_list)\n",
    "\n",
    "def get_correct_term(target_term, valid_words_list):\n",
    "    # Use the first 20,000 entries in the valid word list\n",
    "    valid_words_list = valid_words_list[:20000]\n",
    "\n",
    "    # Store the edit distances in a dictionary\n",
    "    edit_distances = {word: edit_distance(target_term, word) for word in valid_words_list}\n",
    "\n",
    "    # Sort the dictionary in ascending order of edit distances\n",
    "    sorted_distances = sorted(edit_distances.items(), key=lambda x: x[1])\n",
    "\n",
    "    # Return the first entry in the sorted result (minimum edit distance)\n",
    "    return sorted_distances[0][0]\n",
    "\n",
    "def correct_spelling(input_sentence, valid_words_set):\n",
    "    # Tokenize the input sentence after making all terms lowercase\n",
    "    tokenized_sentence = nltk.word_tokenize(input_sentence.lower())\n",
    "\n",
    "    # Correct spelling for each term in the tokenized sentence\n",
    "    corrected_sentence = [term if term in valid_words_set else get_correct_term(term, list(valid_words_set)) for term in tokenized_sentence]\n",
    "\n",
    "    # Return the joined string as output\n",
    "    return ' '.join(corrected_sentence)\n",
    "\n",
    "# Test the function with the input sentence\n",
    "input_sentence = \"The new abacos is great\"\n",
    "valid_words_set = get_valid_words_set()\n",
    "corrected_sentence = correct_spelling(input_sentence, valid_words_set)\n",
    "print(\"Input Sentence:\", input_sentence)\n",
    "print(\"Corrected Sentence:\", corrected_sentence)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
