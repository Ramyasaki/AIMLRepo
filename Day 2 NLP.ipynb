{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b295b42b",
   "metadata": {},
   "source": [
    "# Day 2\n",
    "You are part of a team developing a text classification system for a news aggregator \n",
    "platform. The platform aims to categorize news articles into different topics automatically. \n",
    "The dataset contains news articles along with their corresponding topics. Perform only the \n",
    "Feature extraction techniques.\n",
    "Dataset Link: https://www.kaggle.com/datasets/therohk/million-headlines\n",
    "Data Exploration: Begin by exploring the dataset. What are the different topics/categories \n",
    "present in the dataset? What is the distribution of articles across these topics?\n",
    "Bag-of-Words (BoW): Implement a Bag-of-Words (BoW) model using CountVectorizer \n",
    "or TF-IDF to transform the text data into numerical features. Discuss the advantages and \n",
    "limitations of BoW in this context. Apply both unigram and bigram techniques and \n",
    "compare their effects on classification accuracy.\n",
    "N-grams: Explore the use of N-grams (bi-grams, tri-grams) in feature engineering. How do \n",
    "different N-gram ranges impact the performance of the classification model?\n",
    "TF-IDF: Apply TF-IDF (Term Frequency-Inverse Document Frequency) to the text data. \n",
    "Describe how TF-IDF works and its significance in capturing the importance of words \n",
    "across documents. Compare the results of TF-IDF with the BoW approach.\n",
    "One-Hot Encoding: Investigate the application of One-Hot Encoding to encode categorical \n",
    "variables or labels. Can One-Hot Encoding be used directly for text classification? Why or \n",
    "why not?\n",
    "Deliverables: \n",
    "Present insights gathered from data exploration and discuss the impact of different feature \n",
    "engineering techniques (BoW, N-grams, TF-IDF, One-Hot Encoding). Provide \n",
    "recommendations for the best feature engineering strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f92004",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3396de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of the Dataset:\n",
      "   publish_date                                      headline_text\n",
      "0      20030219  aba decides against community broadcasting lic...\n",
      "1      20030219     act fire witnesses must be aware of defamation\n",
      "2      20030219     a g calls for infrastructure protection summit\n",
      "3      20030219           air nz staff in aust strike for pay rise\n",
      "4      20030219      air nz strike to affect australian travellers\n",
      "\n",
      "Categories present in the dataset:\n",
      "[20030219 20030220 20030221 ... 20211229 20211230 20211231]\n",
      "\n",
      "Distribution of articles across topics:\n",
      "20120824    384\n",
      "20130412    383\n",
      "20110222    380\n",
      "20120814    379\n",
      "20130514    378\n",
      "           ... \n",
      "20210605      6\n",
      "20211023      5\n",
      "20210515      5\n",
      "20210806      1\n",
      "20170209      1\n",
      "Name: publish_date, Length: 6882, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"C://Users//TmC//Downloads//archive//abcnews-date-text.csv\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"Sample of the Dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Explore different topics/categories\n",
    "categories = df['publish_date'].unique()\n",
    "print(\"\\nCategories present in the dataset:\")\n",
    "print(categories)\n",
    "\n",
    "# Distribution of articles across topics\n",
    "print(\"\\nDistribution of articles across topics:\")\n",
    "print(df['publish_date'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b407ef2b",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01933aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['publish_date', 'headline_text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"C://Users//TmC//Downloads//archive//abcnews-date-text.csv\"  # Replace with the actual path to your downloaded CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Check the column names in the DataFrame\n",
    "print(df.columns)\n",
    "\n",
    "# Assuming the column names are 'headline_text' and 'category', adjust accordingly if different\n",
    "# For simplicity, let's focus on a smaller subset for faster execution\n",
    "df = df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['headline_text'], df['publish_date'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Implement Bag-of-Words with unigrams using CountVectorizer\n",
    "count_vectorizer_unigram = CountVectorizer()\n",
    "X_train_counts_unigram = count_vectorizer_unigram.fit_transform(X_train)\n",
    "X_test_counts_unigram = count_vectorizer_unigram.transform(X_test)\n",
    "\n",
    "# Implement Bag-of-Words with bigrams using CountVectorizer\n",
    "count_vectorizer_bigram = CountVectorizer(ngram_range=(2, 2))\n",
    "X_train_counts_bigram = count_vectorizer_bigram.fit_transform(X_train)\n",
    "X_test_counts_bigram = count_vectorizer_bigram.transform(X_test)\n",
    "\n",
    "# Implement Bag-of-Words with unigrams using TF-IDF\n",
    "tfidf_vectorizer_unigram = TfidfVectorizer()\n",
    "X_train_tfidf_unigram = tfidf_vectorizer_unigram.fit_transform(X_train)\n",
    "X_test_tfidf_unigram = tfidf_vectorizer_unigram.transform(X_test)\n",
    "\n",
    "# Implement Bag-of-Words with bigrams using TF-IDF\n",
    "tfidf_vectorizer_bigram = TfidfVectorizer(ngram_range=(2, 2))\n",
    "X_train_tfidf_bigram = tfidf_vectorizer_bigram.fit_transform(X_train)\n",
    "X_test_tfidf_bigram = tfidf_vectorizer_bigram.transform(X_test)\n",
    "\n",
    "# Train a simple classifier (e.g., Multinomial Naive Bayes) and evaluate accuracy\n",
    "# (Continue with the classifier and accuracy evaluation as shown in the previous code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9310392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         publish_date                                      headline_text\n",
      "1144371      20181017  virtual reality trial ahead of fire season in ...\n",
      "282871       20070131                     farmers prepare for ec funding\n",
      "895099       20140810                   the sunday inquisition august 10\n",
      "764744       20130221                                      news csg reax\n",
      "894276       20140806  rosetta spacecraft on final approach to comet ...\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7708a175",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af884eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\TmC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1eda1d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams: [('this',), ('is',), ('a',), ('sample',), ('sentence',)]\n",
      "Bigrams: [('this', 'is'), ('is', 'a'), ('a', 'sample'), ('sample', 'sentence')]\n",
      "Trigrams: [('this', 'is', 'a'), ('is', 'a', 'sample'), ('a', 'sample', 'sentence')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"this is a sample sentence\"\n",
    "\n",
    "# Convert sentence to words\n",
    "words = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Create unigrams, bigrams, and trigrams\n",
    "unigrams = list(ngrams(words, 1))\n",
    "bigrams = list(ngrams(words, 2))\n",
    "trigrams = list(ngrams(words, 3))\n",
    "\n",
    "print(\"Unigrams:\", unigrams)\n",
    "print(\"Bigrams:\", bigrams)\n",
    "print(\"Trigrams:\", trigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe946dd",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0649d3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with TF-IDF (unigram): 0.0019289503295290146\n",
      "Accuracy with BoW (unigram): 0.003777527728660987\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['headline_text'], df['publish_date'], test_size=0.2, random_state=42)\n",
    "\n",
    "# TF-IDF with unigrams\n",
    "tfidf_vectorizer_unigram = TfidfVectorizer()\n",
    "X_train_tfidf_unigram = tfidf_vectorizer_unigram.fit_transform(X_train)\n",
    "X_test_tfidf_unigram = tfidf_vectorizer_unigram.transform(X_test)\n",
    "\n",
    "# Bag-of-Words (BoW) with unigrams using CountVectorizer\n",
    "count_vectorizer_unigram = CountVectorizer()\n",
    "X_train_bow_unigram = count_vectorizer_unigram.fit_transform(X_train)\n",
    "X_test_bow_unigram = count_vectorizer_unigram.transform(X_test)\n",
    "\n",
    "# Train a simple classifier (e.g., Multinomial Naive Bayes) and evaluate accuracy\n",
    "\n",
    "# TF-IDF with unigrams\n",
    "model_tfidf_unigram = MultinomialNB()\n",
    "model_tfidf_unigram.fit(X_train_tfidf_unigram, y_train)\n",
    "y_pred_tfidf_unigram = model_tfidf_unigram.predict(X_test_tfidf_unigram)\n",
    "accuracy_tfidf_unigram = accuracy_score(y_test, y_pred_tfidf_unigram)\n",
    "print(f'Accuracy with TF-IDF (unigram): {accuracy_tfidf_unigram}')\n",
    "\n",
    "# BoW with unigrams\n",
    "model_bow_unigram = MultinomialNB()\n",
    "model_bow_unigram.fit(X_train_bow_unigram, y_train)\n",
    "y_pred_bow_unigram = model_bow_unigram.predict(X_test_bow_unigram)\n",
    "accuracy_bow_unigram = accuracy_score(y_test, y_pred_bow_unigram)\n",
    "print(f'Accuracy with BoW (unigram): {accuracy_bow_unigram}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db3b07e",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd83834",
   "metadata": {},
   "outputs": [],
   "source": [
    "One-Hot Encoding is a technique commonly used for encoding categorical variables or labels.\n",
    "It works by representing each category as a binary vector, where each element in the vector \n",
    "corresponds to a unique category. The element is set to 1 if the data point belongs to that category and 0 otherwise.\n",
    "\n",
    "However, One-Hot Encoding is not directly suitable for text classification tasks. \n",
    "\n",
    "1.High Dimensionality: \n",
    "    In text classification, especially when dealing with a large vocabulary or a large number of \n",
    "    unique words, the One-Hot Encoding would result in a very high-dimensional and sparse vector \n",
    "    representation for each document. This can lead to computational inefficiency and memory issues.\n",
    "\n",
    "2.Loss of Sequence Information: \n",
    "    One-Hot Encoding treats each word independently and doesn't capture the sequential or semantic \n",
    "    relationships between words in a document. It doesn't consider the order or proximity of words, \n",
    "    which is crucial in understanding the meaning of a sentence or document.\n",
    "\n",
    "3.Doesn't Capture Semantic Similarity: \n",
    "     One-Hot Encoding doesn't capture the semantic similarity between words. Words with similar meanings\n",
    "    will be represented as completely independent vectors, which can lead to challenges in understanding \n",
    "    the context and semantics of the text.\n",
    "\n",
    "Instead of One-Hot Encoding, more advanced techniques like Word Embeddings (e.g., Word2Vec, GloVe) or methods \n",
    "based on deep learning architectures (e.g., LSTM, GRU, Transformer models) are commonly used for text classification.\n",
    "These methods provide dense vector representations that capture semantic relationships, maintain word order, \n",
    "and effectively handle high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a64e1e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Category  Category_A  Category_B  Category_C\n",
      "0        A         1.0         0.0         0.0\n",
      "1        B         0.0         1.0         0.0\n",
      "2        A         1.0         0.0         0.0\n",
      "3        C         0.0         0.0         1.0\n",
      "4        B         0.0         1.0         0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TmC\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Sample data with a categorical variable\n",
    "data = {'Category': ['A', 'B', 'A', 'C', 'B']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# One-Hot Encoding\n",
    "encoder = OneHotEncoder(sparse=False)  # Set sparse=False to get a dense array\n",
    "encoded_data = encoder.fit_transform(df[['Category']])\n",
    "\n",
    "# Create a DataFrame with the encoded data\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(['Category']))\n",
    "\n",
    "# Concatenate the original DataFrame with the encoded DataFrame\n",
    "df_encoded = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "print(df_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8198b7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Sample text data\n",
    "text_data = ['This is a positive example', 'This is a negative example', 'Another positive one', 'Negative example here']\n",
    "\n",
    "# Sample labels\n",
    "labels = [10, 10, 0, 1]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# TF-IDF vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Train a classifier (e.g., Naive Bayes)\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
