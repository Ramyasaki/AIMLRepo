{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "from lib.envs.gridworld import GridworldEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Taken from Policy Evaluation Exercise!\n",
    "\n",
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with a random (all 0) value function\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # For each state, perform a \"full backup\"\n",
    "        for s in range(env.nS):\n",
    "            v = 0\n",
    "            # Look at the possible next actions\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                # For each action, look at the possible next states...\n",
    "                for  prob, next_state, reward, done in env.P[s][a]:\n",
    "                    # Calculate the expected value\n",
    "                    v += action_prob * prob * (reward + discount_factor * V[next_state])\n",
    "            # How much our value function changed (across any states)\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        # Stop evaluating once our value function change is below a threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_improvement(env, policy_eval_fn=policy_eval, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Policy Improvement Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI envrionment.\n",
    "        policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Start with a random policy\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    \n",
    "    while True:\n",
    "        # Implement this!\n",
    "        break\n",
    "    \n",
    "    return policy, np.zeros(env.nS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Probability Distribution:\n",
      "[[ 0.25  0.25  0.25  0.25]\n",
      " [ 0.25  0.25  0.25  0.25]\n",
      " [ 0.25  0.25  0.25  0.25]\n",
      " [ 0.25  0.25  0.25  0.25]\n",
      " [ 0.25  0.25  0.25  0.25]\n",
      " [ 0.25  0.25  0.25  0.25]\n",
      " [ 0.25  0.25  0.25  0.25]\n",
      " [ 0.25  0.25  0.25  0.25]\n",
      " [ 0.25  0.25  0.25  0.25]\n",
      " [ 0.25  0.25  0.25  0.25]\n",
      " [ 0.25  0.25  0.25  0.25]\n",
      " [ 0.25  0.25  0.25  0.25]\n",
      " [ 0.25  0.25  0.25  0.25]\n",
      " [ 0.25  0.25  0.25  0.25]\n",
      " [ 0.25  0.25  0.25  0.25]\n",
      " [ 0.25  0.25  0.25  0.25]]\n",
      "\n",
      "Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\n",
      "[[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "\n",
      "Value Function:\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "\n",
      "Reshaped Grid Value Function:\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy, v = policy_improvement(env)\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy:\n",
      "[[0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]]\n",
      "Optimal Value Function:\n",
      "[944.72357234 864.01312811 903.55729147 873.75063348 789.53799752\n",
      " 864.01312811 789.53799565 816.76688902 864.01312999 826.02716258\n",
      " 903.55729147 835.38097132 807.59922297 826.02716258 807.59922109\n",
      " 873.75063348 955.27633662 873.75063542 913.69423476 883.58649945\n",
      " 934.27633662 854.37299683 893.52171855 864.01312715 798.52323074\n",
      " 873.75063542 798.52322888 826.02716161 854.37299869 816.76689095\n",
      " 893.52171855 826.02716161 816.76689281 835.38097326 816.76689095\n",
      " 883.58649945 944.72357326 883.58650137 903.55729242 893.5217176\n",
      " 883.58650321 807.59922204 844.82926686 816.76688999 844.8292687\n",
      " 923.93357142 844.82926686 873.75063446 844.8292687  807.59922204\n",
      " 883.58650137 816.76688999 826.02716536 844.82926686 826.02716353\n",
      " 893.5217176  893.52172131 934.2763357  893.52171949 903.55729147\n",
      " 873.75063818 798.52322982 835.3809742  807.59922109 854.37299959\n",
      " 934.2763357  854.37299777 883.58650042 835.38097602 798.52322982\n",
      " 873.75063635 807.59922109 835.38097602 854.37299777 835.3809742\n",
      " 903.55729147 883.5865041  944.72357234 883.5865023  913.69423476\n",
      " 864.01313179 789.53799752 826.02716445 798.52322888 864.01313179\n",
      " 944.72357234 864.01312999 893.52171855 826.02716626 789.53799752\n",
      " 864.01312999 798.52322888 826.02716626 844.82926779 826.02716445\n",
      " 893.52171855 873.75063906 955.27633662 873.75063727 903.55729242\n",
      " 934.27633662 854.37299683 893.52171855 864.01312715 798.52323074\n",
      " 873.75063542 798.52322888 826.02716161 873.75063727 835.38097326\n",
      " 913.69423476 844.82926495 816.76689281 835.38097326 816.76689095\n",
      " 883.58649945 944.72357326 883.58650137 923.93357142 893.5217176\n",
      " 923.93357326 844.82926686 883.58650137 854.37299588 807.59922388\n",
      " 883.58650137 807.59922204 835.3809723  864.0131309  826.02716353\n",
      " 903.55729242 835.3809723  826.02716536 844.82926686 826.02716353\n",
      " 893.5217176  934.27633752 893.52171949 913.6942357  903.55729147\n",
      " 893.52172131 816.76689189 854.37299777 826.02716258 835.38097602\n",
      " 913.6942357  835.3809742  864.01312811 854.37299959 816.76689189\n",
      " 893.52171949 826.02716258 835.38097602 854.37299777 835.3809742\n",
      " 903.55729147 903.55729515 923.93357234 903.55729334 913.69423476\n",
      " 883.5865041  807.59922297 844.82926779 816.76689095 844.8292696\n",
      " 923.93357234 844.82926779 873.75063542 844.8292696  807.59922297\n",
      " 883.5865023  816.76689095 844.8292696  864.01312999 844.82926779\n",
      " 913.69423476 893.5217222  934.27633662 893.52172041 923.93357142\n",
      " 873.75063906 798.52323074 835.38097512 807.59922204 854.37300048\n",
      " 934.27633662 854.37299869 883.58650137 835.3809769  798.52323074\n",
      " 873.75063727 807.59922204 835.3809769  854.37299869 835.38097512\n",
      " 903.55729242 883.58650497 944.72357326 883.58650321 913.6942357\n",
      " 923.93357326 844.82926686 883.58650137 854.37299588 807.59922388\n",
      " 883.58650137 807.59922204 835.3809723  883.58650321 844.82926686\n",
      " 923.93357142 854.37299588 826.02716536 844.82926686 826.02716353\n",
      " 893.5217176  934.27633752 893.52171949 934.2763357  903.55729147\n",
      " 913.69423752 835.3809742  873.75063635 844.82926592 816.76689371\n",
      " 893.52171949 816.76689189 844.82926592 873.75063818 835.3809742\n",
      " 913.6942357  844.82926592 835.38097602 854.37299777 835.3809742\n",
      " 903.55729147 923.93357415 903.55729334 923.93357234 913.69423476\n",
      " 903.55729515 826.02716445 864.01312999 835.38097326 826.02716626\n",
      " 903.55729334 826.02716445 854.37299683 864.01313179 826.02716445\n",
      " 903.55729334 835.38097326 844.8292696  864.01312999 844.82926779\n",
      " 913.69423476 913.69423841 913.69423662 913.69423662 923.93357142\n",
      " 893.5217222  816.76689281 854.37299869 826.02716353 835.3809769\n",
      " 913.69423662 835.38097512 864.01312906 854.37300048 816.76689281\n",
      " 893.52172041 826.02716353 854.37300048 873.75063727 854.37299869\n",
      " 923.93357142 903.55729602 923.93357326 903.55729426 934.2763357\n",
      " 883.58650497 807.59922388 844.8292687  816.76689189 844.82927047\n",
      " 923.93357326 844.8292687  873.75063635 844.82927047 807.59922388\n",
      " 883.58650321 816.76689189 844.82927047 864.0131309  844.8292687\n",
      " 913.6942357  893.52172306 934.27633752 893.52172131 923.93357234\n",
      " 913.69423752 835.3809742  873.75063635 844.82926592 798.52323164\n",
      " 873.75063635 798.52322982 826.02716258 893.52172131 854.37299777\n",
      " 934.2763357  864.01312811 816.76689371 835.3809742  816.76689189\n",
      " 883.58650042 923.93357415 883.5865023  944.72357234 893.52171855\n",
      " 903.55729515 826.02716445 864.01312999 835.38097326 807.59922477\n",
      " 883.5865023  807.59922297 835.38097326 864.01313179 826.02716445\n",
      " 903.55729334 835.38097326 826.02716626 844.82926779 826.02716445\n",
      " 893.52171855 913.69423841 893.52172041 913.69423662 903.55729242\n",
      " 893.5217222  816.76689281 854.37299869 826.02716353 816.76689459\n",
      " 893.52172041 816.76689281 844.82926686 854.37300048 816.76689281\n",
      " 893.52172041 826.02716353 835.3809769  854.37299869 835.38097512\n",
      " 903.55729242 903.55729602 903.55729426 903.55729426 913.6942357\n",
      " 883.58650497 807.59922388 844.8292687  816.76689189 826.02716713\n",
      " 903.55729426 826.02716536 854.37299777 844.82927047 807.59922388\n",
      " 883.58650321 816.76689189 864.01313267 883.58650321 864.0131309\n",
      " 934.2763357  893.52172306 913.69423752 893.52172131 944.72357234\n",
      " 873.75063992 798.52323164 835.38097602 807.59922297 835.38097777\n",
      " 913.69423752 835.38097602 864.01312999 835.38097777 798.52323164\n",
      " 873.75063818 807.59922297 854.37300134 873.75063818 854.37299959\n",
      " 923.93357234 883.58650583 923.93357415 883.5865041  934.27633662\n",
      " 903.55729515 826.02716445 864.01312999 835.38097326 789.53799933\n",
      " 864.01312999 789.53799752 816.76689095 903.55729515 864.01312999\n",
      " 944.72357234 873.75063542 807.59922477 826.02716445 807.59922297\n",
      " 873.75063542 913.69423841 873.75063727 955.27633662 883.58650137\n",
      " 893.5217222  816.76689281 854.37299869 826.02716353 798.52323253\n",
      " 873.75063727 798.52323074 826.02716353 854.37300048 816.76689281\n",
      " 893.52172041 826.02716353 816.76689459 835.38097512 816.76689281\n",
      " 883.58650137 903.55729602 883.58650321 903.55729426 893.52171949\n",
      " 883.58650497 807.59922388 844.8292687  816.76689189 807.59922565\n",
      " 883.58650321 807.59922388 835.3809742  844.82927047 807.59922388\n",
      " 883.58650321 816.76689189 826.02716713 844.8292687  826.02716536\n",
      " 893.52171949 893.52172306 893.52172131 893.52172131 903.55729334\n",
      " 873.75063992 798.52323164 835.38097602 807.59922297 816.76689546\n",
      " 893.52172131 816.76689371 844.82926779 835.38097777 798.52323164\n",
      " 873.75063818 807.59922297 873.75063992 893.52172131 873.75063818\n",
      " 944.72357234 883.58650583 903.55729515 883.5865041  955.27633662\n",
      " 864.01313352 789.53799933 826.02716626 798.52323074 826.02716799\n",
      " 903.55729515 826.02716626 854.37299869 826.02716799 789.53799933\n",
      " 864.01313179 798.52323074 864.01313352 883.5865041  864.01313179\n",
      " 934.27633662 873.75064077 913.69423841 873.75063906 944.72357326]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "def policy_iteration(env, gamma=0.99, theta=1e-6):\n",
    "    nS = env.observation_space.n  # Number of states\n",
    "    nA = env.action_space.n  # Number of actions\n",
    "\n",
    "    # Initialize the policy arbitrarily\n",
    "    policy = np.ones([nS, nA]) / nA\n",
    "\n",
    "    while True:\n",
    "        # Policy Evaluation (Calculate the state-value function V_pi)\n",
    "        V = np.zeros(nS)\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(nS):\n",
    "                v = V[s]\n",
    "                weighted_rewards = [sum(p * (r + gamma * V[s_next]) for p, s_next, r, _ in env.P[s][a]) for a in range(nA)]\n",
    "                V[s] = sum(policy[s][a] * weighted_rewards[a] for a in range(nA))\n",
    "                delta = max(delta, abs(v - V[s]))\n",
    "            if delta < theta:\n",
    "                break\n",
    "\n",
    "        policy_stable = True\n",
    "\n",
    "        # Policy Improvement\n",
    "        for s in range(nS):\n",
    "            old_action = np.argmax(policy[s])\n",
    "            action_values = [sum(p * (r + gamma * V[s_next]) for p, s_next, r, _ in env.P[s][a]) for a in range(nA)]\n",
    "            best_action = np.argmax(action_values)\n",
    "\n",
    "            # Update the policy\n",
    "            new_policy = np.zeros(nA)\n",
    "            new_policy[best_action] = 1\n",
    "            policy[s] = new_policy\n",
    "\n",
    "            if old_action != best_action:\n",
    "                policy_stable = False\n",
    "\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    return policy, V\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('Taxi-v3')\n",
    "\n",
    "# Run Policy Iteration\n",
    "optimal_policy, optimal_value = policy_iteration(env)\n",
    "\n",
    "# Print the optimal policy and value function\n",
    "print(\"Optimal Policy:\")\n",
    "print(optimal_policy)\n",
    "\n",
    "print(\"Optimal Value Function:\")\n",
    "print(optimal_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
